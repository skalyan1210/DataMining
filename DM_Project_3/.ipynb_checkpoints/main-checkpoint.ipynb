{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "from random import randrange\n",
    "from pprint import pprint\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 31)\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('project3_dataset1.txt', sep=\"\\t\", header=None) #importing the dataset\n",
    "nrows = len(dataset) #global variable\n",
    "ncol = len(dataset.columns) #global variable\n",
    "dataset= np.array(dataset)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_termination(data):\n",
    "#this function checks whether all the data points in the dataset are of the same class or not.\n",
    "#it checks whether the final row(classes) are all 0 or 1 and returns the boolean value accordingly.\n",
    "    global ncol\n",
    "    if all(data[:,ncol-1]==1)== True or all(data[:,ncol-1]==0)== True:\n",
    "        return True\n",
    "    else:\n",
    "        return False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(data):\n",
    "    #returns the class value (used when there is termination of the leaf)\n",
    "    global ncol\n",
    "    return np.unique(data[:,ncol-1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, column, svalue):\n",
    "#using this function we split the data into two subtrees (left,right).\n",
    "#Here we use a global variable(indices) which is a list consisting of the column numbers that are categorical.\n",
    "    global indices\n",
    "    if column in indices:\n",
    "        left= data[data[:,column] == svalue]\n",
    "        right= data[data[:,column] !=  svalue]  \n",
    "    else:\n",
    "        left= data[data[:,column] <= svalue]\n",
    "        right= data[data[:,column] >  svalue]\n",
    "    return left,right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cat(data):\n",
    "    #this function finds the columns that are having the categorical values\n",
    "    cat = []\n",
    "    for i in range(len(data[0])-1):\n",
    "            value = data[0][i]\n",
    "            if (isinstance(value, str)):\n",
    "                cat.append(i)\n",
    "    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain(left,right):\n",
    "    #this function calculates the overall gain by calculating the gini index of the left and right subtrees\n",
    "    global ncol\n",
    "    classes, count_l = np.unique(left[:,ncol-1], return_counts=True)\n",
    "    classes, count_r = np.unique(right[:,ncol-1], return_counts=True)\n",
    "    prob_l = count_l / count_l.sum()\n",
    "    prob_r = count_r / count_r.sum()\n",
    "    gini_l = 1 - np.sum(np.power(prob_l,2))\n",
    "    gini_r = 1 - np.sum(np.power(prob_r,2))\n",
    "    gain = ((gini_l * len(left)) + (gini_r* len(right)))/ (len(left)+len(right))\n",
    "    return gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split(data):\n",
    "    #in this function we find the best split i.e on which condition the parent node should be divided into its child nodes\n",
    "    max_gain =1\n",
    "    for i in range(len(data[0])-1): #we traverse through all the columns\n",
    "         for j in np.unique(data[:,i]):  #considering all the unique values present in that certain column\n",
    "                left,right = split_data(data,i,j) #we split the node into its subtrees\n",
    "                cur_gain = gain(left,right)  #generate the gain\n",
    "                if cur_gain <= max_gain:\n",
    "                    max_gain = cur_gain\n",
    "                    scolumn = i\n",
    "                    svalue = j\n",
    "    return scolumn, svalue \n",
    "#return the column number and the value which constitute to the low gini index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(dataset):\n",
    "    global indices\n",
    "    data = dataset \n",
    "    if (check_termination(data)): #check whether the tree has all the values in the same class\n",
    "        return classify(data) #if yes then return the class number of the tree\n",
    "    else:    \n",
    "        scolumn, svalue = best_split(data) #send the data to check the best slipt\n",
    "        left, right = split_data(data, scolumn, svalue) #split the data into left and right trees according to the splits\n",
    "        left_tree = decision_tree(left) #send the left tree to the function again to find the subtrees of it until the termination criteria is satisfied.\n",
    "        right_tree = decision_tree(right) #same for the right tree\n",
    "        if scolumn not in indices:   #save the column number and the value by which the tree was split.\n",
    "            condition = str(scolumn)+\" <= \"+str(svalue)\n",
    "        else:\n",
    "            condition = str(scolumn)+\" == \"+str(svalue)\n",
    "        sub_tree = {condition: []}\n",
    "        sub_tree[condition].append(left_tree)\n",
    "        sub_tree[condition].append(right_tree)\n",
    "        return sub_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'20 <= 16.77': [{'27 <= 0.1357': [{'29 <= 0.05504': [1.0, {'13 <= 38.34': [{'14 <= 0.00328': [{'27 <= 0.09077': [0.0, 1.0]}, {'21 <= 33.17': [0.0, {'21 <= 33.37': [1.0, 0.0]}]}]}, {'28 <= 0.1978': [1.0, {'27 <= 0.1108': [0.0, 1.0]}]}]}]}, {'21 <= 25.5': [{'23 <= 808.2': [{'24 <= 0.17800000000000002': [0.0, 1.0]}, {'24 <= 0.1294': [0.0, 1.0]}]}, {'7 <= 0.053810000000000004': [{'21 <= 27.2': [0.0, 1.0]}, 1.0]}]}]}, {'21 <= 19.58': [{'27 <= 0.1452': [0.0, 1.0]}, {'24 <= 0.08774': [0.0, {'26 <= 0.1663': [{'28 <= 0.2394': [0.0, 1.0]}, 1.0]}]}]}]}\n"
     ]
    }
   ],
   "source": [
    "indices = find_cat(dataset) #save the indices where the categorical values are present.\n",
    "tree = decision_tree(dataset)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(data, tree): #classify the data into 1 or 0 according to the tree\n",
    "    condition = list(tree.keys())[0] #get the key of the tree(condition)\n",
    "    column, operator, value = condition.split(\" \")\n",
    "    if operator == \"<=\":    #for continuous variables\n",
    "        if data[int(column)] <= float(value):\n",
    "            subtree = tree[condition][0]\n",
    "        else:\n",
    "            subtree = tree[condition][1]\n",
    "    else:       #for categorial variables\n",
    "        if str(data[int(column)]) == value:\n",
    "            subtree = tree[condition][0]\n",
    "        else:\n",
    "            subtree = tree[condition][1]\n",
    "    if not isinstance(subtree, dict):\n",
    "        return subtree\n",
    "    else:\n",
    "        return predictions(data,subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(actual, predicted):\n",
    "    actual = list(actual)\n",
    "    predicted = list(predicted)\n",
    "    cm = confusion_matrix(actual,predicted)\n",
    "    TN = cm[0][0]\n",
    "    TP = cm[1][1]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "    return TP,FN,FP,TN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333333333333333\n",
      "Precision: 0.88\n",
      "Recall: 0.9166666666666666\n",
      "F-1 Measure: 0.8979591836734694\n"
     ]
    }
   ],
   "source": [
    "tree= decision_tree(dataset[2:200])\n",
    "p = []\n",
    "test = dataset[150:300]\n",
    "o = test[:,-1]\n",
    "for row in range(len(test)):\n",
    "        p.append(predictions(test[row],tree))\n",
    "TP,FN,FP,TN  = evaluation(np.asarray(p),o)\n",
    "a= float(TP + TN) / (TP+FN+FP+TN)\n",
    "p= float(TP) / (TP + FP)\n",
    "r= float(TP) / (TP + FN)\n",
    "fm= float(2 * TP) / ((2 * TP) + FN + FP)\n",
    "\n",
    "print(\"Accuracy: \" + str(a))\n",
    "print(\"Precision: \" + str(p))\n",
    "print(\"Recall: \" + str(r))\n",
    "print(\"F-1 Measure: \" + str(fm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree CrossValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9226817042606514\n",
      "Precision: 0.8853297012584938\n",
      "Recall: 0.9148593993252023\n",
      "F-1 Measure: 0.8964835437388903\n"
     ]
    }
   ],
   "source": [
    "cross_valid = np.array_split(dataset,10)\n",
    "a= p = r = fm = 0\n",
    "for i in range(len(cross_valid)):\n",
    "    test = cross_valid[i]\n",
    "    train = np.vstack([x for j, x in enumerate(cross_valid) if j != i])\n",
    "    test_predictions= []\n",
    "    tree = decision_tree(train)\n",
    "    for row in range(len(test)):\n",
    "        test_predictions.append(predictions(test[row],tree))\n",
    "    TP,FN,FP,TN  = evaluation(np.array(test[:, -1]), np.asarray(test_predictions))\n",
    "    a+= float(TP + TN) / (TP+FN+FP+TN)\n",
    "    p+= float(TP) / (TP + FP)\n",
    "    r+= float(TP) / (TP + FN)\n",
    "    fm+= float(2 * TP) / ((2 * TP) + FN + FP)\n",
    "\n",
    "print(\"Accuracy: \" + str(a/10))\n",
    "print(\"Precision: \" + str(p/10))\n",
    "print(\"Recall: \" + str(r/10))\n",
    "print(\"F-1 Measure: \" + str(fm/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_list(n,ncol):\n",
    "    l=[]\n",
    "    for i in range(n):\n",
    "        l.append(random.randint(0,ncol))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split_new(data,n):\n",
    "    max_gain =10\n",
    "    global ncol\n",
    "    l= random_list(n,ncol-1) #generate n random column indexes\n",
    "    for i in l:\n",
    "         for j in np.unique(data[:,i]):\n",
    "                left,right = split_data(data,i,j)\n",
    "                cur_gain = gain(left,right)\n",
    "                if cur_gain <= max_gain:\n",
    "                    max_gain = cur_gain\n",
    "                    scolumn = i\n",
    "                    svalue = j\n",
    "    return scolumn, svalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_new(dataset,n):\n",
    "    global indices\n",
    "    data = dataset \n",
    "    if (check_termination(data)):\n",
    "        return classify(data)\n",
    "    else:    \n",
    "        scolumn, svalue = best_split_new(data,n)\n",
    "        left, right = split_data(data, scolumn, svalue)\n",
    "        left_tree = decision_tree_new(left,n)\n",
    "        right_tree = decision_tree_new(right,n)\n",
    "        if scolumn not in indices:\n",
    "            condition = str(scolumn)+\" <= \"+str(svalue)\n",
    "        else:\n",
    "            condition = str(scolumn)+\" == \"+str(svalue)\n",
    "        sub_tree = {condition: []}\n",
    "        sub_tree[condition].append(left_tree)\n",
    "        sub_tree[condition].append(right_tree)\n",
    "        return sub_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(dataset,test,T):\n",
    "    global ncol\n",
    "    org_values =[]\n",
    "    pred_values = []\n",
    "    for i in range(T):\n",
    "        size = random.randint(50,int(nrows/3))\n",
    "        N = random.randint(0,int(nrows/2))\n",
    "        train = dataset[N:N+size]\n",
    "        m = random.randint(5,int(ncol)-int(ncol/5))\n",
    "        values = test[:,-1]\n",
    "        tree = decision_tree_new(train,m)\n",
    "        predictions_r =[]\n",
    "        for row in range(len(test)):\n",
    "            predictions_r.append(predictions(test[row],tree))\n",
    "        org_values=values\n",
    "        pred_values.append(predictions_r)\n",
    "    return org_values,pred_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_new(p):\n",
    "    pre =[]\n",
    "    for i in p:\n",
    "        class_r,count=np.unique(i,return_counts=True)\n",
    "        pre.append(class_r[count.argmax()])\n",
    "    return pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F-1 Measure: 1.0\n"
     ]
    }
   ],
   "source": [
    "o,p = random_forest(dataset[2:300],dataset[300:360],10)\n",
    "TP,FN,FP,TN  = evaluation(np.asarray(predicted_new(np.array(np.matrix(p)).T)),o)\n",
    "a= float(TP + TN) / (TP+FN+FP+TN)\n",
    "p= float(TP) / (TP + FP)\n",
    "r= float(TP) / (TP + FN)\n",
    "fm= float(2 * TP) / ((2 * TP) + FN + FP)\n",
    "\n",
    "print(\"Accuracy: \" + str(a))\n",
    "print(\"Precision: \" + str(p))\n",
    "print(\"Recall: \" + str(r))\n",
    "print(\"F-1 Measure: \" + str(fm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests CrossValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_valid = np.array_split(dataset,10)\n",
    "a= p = r = fm = 0\n",
    "for i in range(len(cross_valid)):\n",
    "    test = cross_valid[i]\n",
    "    train = np.vstack([x for j, x in enumerate(cross_valid) if j != i])\n",
    "    test_predictions= []\n",
    "    tree = decision_tree(train)\n",
    "    org,pred = random_forest(train,test,7)\n",
    "    TP,FN,FP,TN  = evaluation(np.asarray(predicted_new(np.array(np.matrix(pred)).T)),org)\n",
    "    a+= float(TP + TN) / (TP+FN+FP+TN)\n",
    "    p+= float(TP) / (TP + FP)\n",
    "    r+= float(TP) / (TP + FN)\n",
    "    fm+= float(2 * TP) / ((2 * TP) + FN + FP)\n",
    "\n",
    "print(\"Accuracy: \" + str(a/10))\n",
    "print(\"Precision: \" + str(p/10))\n",
    "print(\"Recall: \" + str(r/10))\n",
    "print(\"F-1 Measure: \" + str(fm/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_func(predictions,actual,weights):\n",
    "    r = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] != actual[i]:\n",
    "            r += weights[i]\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_boost(data, tree):\n",
    "    condition = list(tree.keys())[0]\n",
    "    column, operator, value = condition.split(\" \")\n",
    "    if operator == \"<=\":\n",
    "        if data[int(column)] <= float(value):\n",
    "            subtree = tree[condition][0]\n",
    "        else:\n",
    "            subtree = tree[condition][1]\n",
    "    else:\n",
    "        if str(data[int(column)]) == value:\n",
    "            subtree = tree[condition][0]\n",
    "        else:\n",
    "            subtree = tree[condition][1]\n",
    "    if not isinstance(subtree, dict):\n",
    "        return subtree\n",
    "    else:\n",
    "        return predictions(data,subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_boost(dataset):\n",
    "    global indices\n",
    "    data = dataset \n",
    "    if (check_termination(data)): #check whether the tree has all the values in the same class\n",
    "        return classify(data) #if yes then return the class number of the tree\n",
    "    else:    \n",
    "        scolumn, svalue = best_split(data) #send the data to check the best slipt\n",
    "        left, right = split_data(data, scolumn, svalue) #split the data into left and right trees according to the splits\n",
    "        left_tree = decision_tree(left) #send the left tree to the function again to find the subtrees of it until the termination criteria is satisfied.\n",
    "        right_tree = decision_tree(right) #same for the right tree\n",
    "        if scolumn not in indices:   #save the column number and the value by which the tree was split.\n",
    "            condition = str(scolumn)+\" <= \"+str(svalue)\n",
    "        else:\n",
    "            condition = str(scolumn)+\" == \"+str(svalue)\n",
    "        sub_tree = {condition: []}\n",
    "        sub_tree[condition].append(left_tree)\n",
    "        sub_tree[condition].append(right_tree)\n",
    "        return sub_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_new_boost(data, trees, alpha_values):\n",
    "    predictions = []\n",
    "    for i in range(len(data)):\n",
    "        zero = 0\n",
    "        one = 0\n",
    "        for j in range(len(trees)):\n",
    "            alpha = alpha_values[j]\n",
    "            prediction = predictions_boost(data[i],trees[j])\n",
    "            if prediction == 0:\n",
    "                zero += alpha\n",
    "            else:\n",
    "                one += alpha\n",
    "        if one > zero:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting CrossValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_boost=dataset\n",
    "num_bags =5\n",
    "cross_valid_boost= np.array_split(dataset,10)\n",
    "folds = len(cross_valid_boost)\n",
    "size = int(len(dataset_boost)/ folds)\n",
    "a= p = r = fm = 0\n",
    "for i in range(folds):\n",
    "    td_start = (i * size)\n",
    "    td_end = td_start + size\n",
    "    td_id = set(range(td_start, td_end))\n",
    "    trd_id = set(range(0,nrows))-td_id \n",
    "    t_data= cross_valid_boost[i] #testing data\n",
    "    tr_data = np.vstack([x for j, x in enumerate(cross_valid_boost) if j != i]) #training data\n",
    "    weights = [1 / len(trd_id) for x in range(len(trd_id))]\n",
    "    trees = []\n",
    "    alpha_values = []\n",
    "    for j in range(num_bags):\n",
    "        error = 9999\n",
    "        tree=None\n",
    "        while error > 0.5:\n",
    "            new_train_id = np.random.choice(list(trd_id), len(tr_data), replace=True, p=weights)\n",
    "            new_train_data = dataset_boost[new_train_id]\n",
    "            tree = decision_tree_boost(new_train_data)\n",
    "            predictions_b = []\n",
    "            for k in range(len(tr_data)):\n",
    "                predictions_b.append(predictions_boost(tr_data[k], tree))\n",
    "            error = error_func(predictions_b, tr_data[:,-1], weights)\n",
    "        alpha = (1/2)*math.log((1-error)/error)\n",
    "        alpha_values.append(alpha)\n",
    "        trees.append(tree)\n",
    "        for k in range(len(predictions_b)):\n",
    "            actual = tr_data[k][len(tr_data[0])-1]\n",
    "            predicted_label = predictions_b[k]\n",
    "            if actual != predicted_label:\n",
    "                weights[k] *= math.exp(-1 * alpha * - 1)\n",
    "            else:\n",
    "                weights[k] *= math.exp(-1 * alpha * + 1)\n",
    "        new_sum = np.sum(weights)\n",
    "        for k in range(len(weights)):\n",
    "            weights[k] /= new_sum\n",
    "    predictions_b = predictions_new_boost(t_data, trees, alpha_values)\n",
    "    org = t_data[:,-1]\n",
    "    TP,FN,FP,TN  = evaluation(np.asarray(predictions_b),org)\n",
    "    a+= float(TP + TN) / (TP+FN+FP+TN)\n",
    "    p+= float(TP) / (TP + FP)\n",
    "    r+= float(TP) / (TP + FN)\n",
    "    fm+= float(2 * TP) / ((2 * TP) + FN + FP)\n",
    "\n",
    "print(\"Accuracy: \" + str(a*10))\n",
    "print(\"Precision: \" + str(p * 10))\n",
    "print(\"Recall: \" + str(r* 10))\n",
    "print(\"F-1 Measure: \" + str(fm* 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
